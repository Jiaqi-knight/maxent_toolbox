
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>maxent_example</title><meta name="generator" content="MATLAB 9.3"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2018-07-09"><meta name="DC.source" content="maxent_example.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#2">part 1: working with small distributions of neurons (exhaustively)</a></li><li><a href="#3">part 2: working with larger distributions of neurons (MCMC)</a></li><li><a href="#4">part 3: working with RP (random projection) models</a></li><li><a href="#5">part 4: specifying a custom list of correlations</a></li><li><a href="#6">part 5: constructing composite models</a></li><li><a href="#7">part 6: constructing and sampling from high order Markov chains</a></li></ul></div><pre class="codeinput"><span class="comment">% maxent_example.m</span>
<span class="comment">%</span>
<span class="comment">% Example code for the maximum entropy toolkit</span>
<span class="comment">% Ori Maoz, July 2016:  orimaoz@gmail.com,</span>


<span class="comment">%</span>
<span class="comment">% Note: all the "maxent." prefixes before the function calls can be omitted by commenting out the following line:</span>
<span class="comment">%import maxent.*</span>
</pre><h2 id="2">part 1: working with small distributions of neurons (exhaustively)</h2><pre class="codeinput"><span class="comment">% load spiking data of 15 neurons</span>
load <span class="string">example15</span>

<span class="comment">% randomly divide it into a training set and a test set (so we can verify how well we trained)</span>
[ncells,nsamples] = size(spikes15);
idx_train = randperm(nsamples,ceil(nsamples/2));
idx_test = setdiff(1:nsamples,idx_train);
samples_train = spikes15(:,idx_train);
samples_test = spikes15(:,idx_test);

<span class="comment">% create a k-pairwise model (pairwise maxent with synchrony constraints)</span>
model = maxent.createModel(ncells,<span class="string">'kising'</span>);

<span class="comment">% train the model to a threshold of one standard deviation from the error of computing the marginals.</span>
<span class="comment">% because the distribution is relatively small (15 dimensions) we can explicitly represent all 2^15 states</span>
<span class="comment">% in memory and train the model in an exhaustive fashion.</span>
model = maxent.trainModel(model,samples_train,<span class="string">'threshold'</span>,1);

<span class="comment">% now check the kullback-leibler divergence between the model predictions and the pattern counts in the test-set</span>
empirical_distribution = maxent.getEmpiricalModel(samples_test);
model_logprobs = maxent.getLogProbability(model,empirical_distribution.words);
test_dkl = maxent.dkl(empirical_distribution.logprobs,model_logprobs);
fprintf(<span class="string">'Kullback-Leibler divergence from test set: %f\n'</span>,test_dkl);

model_entropy = maxent.getEntropy(model);
fprintf(<span class="string">'Model entropy: %.03f   empirical dataset entropy: %.03f\n'</span>, model_entropy, empirical_distribution.entropy);

<span class="comment">% get the marginals (firing rates and correlations) of the test data and see how they compare to the model predictions</span>
marginals_data = maxent.getEmpiricalMarginals(samples_test,model);
marginals_model = maxent.getMarginals(model);

<span class="comment">% plot them on a log scale</span>
figure
loglog(marginals_data,marginals_model,<span class="string">'b*'</span>);
hold <span class="string">on</span>;
minval = min([marginals_data(marginals_data&gt;0)]);
plot([minval 1],[minval 1],<span class="string">'-r'</span>); <span class="comment">% identity line</span>
xlabel(<span class="string">'empirical marginal'</span>);
ylabel(<span class="string">'predicted marginal'</span>);
title(sprintf(<span class="string">'marginals in %d cells'</span>,ncells));
</pre><pre class="codeoutput">Training to threshold: 1.000 standard deviations
Maximum MSE: 1.000
converged (marginals match)
Standard deviations from marginals: 0.090 (mean), 0.968 (max) [9]  DKL: 0.097
Kullback-Leibler divergence from test set: 0.107325
Model entropy: 6.470   empirical dataset entropy: 6.369
</pre><img vspace="5" hspace="5" src="maxent_example_01.png" alt=""> <h2 id="3">part 2: working with larger distributions of neurons (MCMC)</h2><pre class="codeinput">load <span class="string">example50</span>

<span class="comment">% randomly divide into train/test sets</span>
[ncells,nsamples] = size(spikes50);
idx_train = randperm(nsamples,ceil(nsamples/2));
idx_test = setdiff(1:nsamples,idx_train);
samples_train = spikes50(:,idx_train);
samples_test = spikes50(:,idx_test);

<span class="comment">% create a pairwise maximum entropy model</span>
model = maxent.createModel(50,<span class="string">'pairwise'</span>);

<span class="comment">% train the model to a threshold of 1.5 standard deviations from the error of computing the marginals.</span>
<span class="comment">% because the distribution is larger (50 dimensions) we cannot explicitly iterate over all 5^20 states</span>
<span class="comment">% in memory and will use markov chain monte carlo (MCMC) methods to obtain an approximation</span>
model = maxent.trainModel(model,samples_train,<span class="string">'threshold'</span>,1.5);


<span class="comment">% get the marginals (firing rates and correlations) of the test data and see how they compare to the model predictions.</span>
<span class="comment">% here the model marginals could not be computed exactly so they will be estimated using monte-carlo. We specify the</span>
<span class="comment">% number of samples we use so that their estimation will have the same amoutn noise as the empirical marginal values</span>
marginals_data = maxent.getEmpiricalMarginals(samples_test,model);
marginals_model = maxent.getMarginals(model,<span class="string">'nsamples'</span>,size(samples_test,2));

<span class="comment">% plot them on a log scale</span>
figure
loglog(marginals_data,marginals_model,<span class="string">'b*'</span>);
hold <span class="string">on</span>;
minval = min([marginals_data(marginals_data&gt;0)]);
plot([minval 1],[minval 1],<span class="string">'-r'</span>); <span class="comment">% identity line</span>
xlabel(<span class="string">'empirical marginal'</span>);
ylabel(<span class="string">'predicted marginal'</span>);
title(sprintf(<span class="string">'marginals in %d cells'</span>,ncells));

<span class="comment">% the model that the MCMC solver returns is not normalized. If we want to compare the predicted and actual probabilities</span>
<span class="comment">% of individual firing patterns, we will need to first normalize the model. We will use the wang-landau algorithm for</span>
<span class="comment">% this. We chose parameters which are less strict than the default settings so that we will have a faster runtime.</span>
disp(<span class="string">'Normalizing model...'</span>);
model = maxent.wangLandau(model,<span class="string">'binsize'</span>,0.1,<span class="string">'depth'</span>,15);

<span class="comment">% the normalization factor was added to the model structure. Now that we have a normalized model, we'll use it to</span>
<span class="comment">% predict the frequency of activity patterns. We will start by observing all the patterns that repeated at least twice</span>
<span class="comment">% (because a pattern that repeated at least once may grossly overrepresent its probability and is not meaningful in this</span>
<span class="comment">% sort of analysis)</span>
limited_empirical_distribution = maxent.getEmpiricalModel(samples_test,<span class="string">'min_count'</span>,2);


<span class="comment">% get the model predictions for these patterns</span>
model_logprobs = maxent.getLogProbability(model,limited_empirical_distribution.words);

<span class="comment">% nplot on a log scale</span>
figure
plot(limited_empirical_distribution.logprobs,model_logprobs,<span class="string">'bo'</span>);
hold <span class="string">on</span>;
minval = min(limited_empirical_distribution.logprobs);
plot([minval 0],[minval 0],<span class="string">'-r'</span>);  <span class="comment">% identity line</span>
xlabel(<span class="string">'empirical pattern log frequency'</span>);
ylabel(<span class="string">'predicted pattern log frequency'</span>);
title(sprintf(<span class="string">'activity pattern frequency in %d cells'</span>,ncells));



<span class="comment">% Wang-landau also approximated the model entropy, let's compare it to the entropy of the empirical dataset.</span>
<span class="comment">% for this we want to look at the entire set, not just the set limited repeating patterns</span>
empirical_distribution = maxent.getEmpiricalModel(samples_test);

<span class="comment">% it will not be surprising to see that the empirical entropy is much lower than the model, this is because the</span>
<span class="comment">% distribution is very undersampled</span>
fprintf(<span class="string">'Model entropy: %.03f bits, empirical entropy (test set): %.03f bits\n'</span>,model.entropy,empirical_distribution.entropy);

<span class="comment">% generate samples from the distribution and compute their entropy. This should give a result which is must closer to</span>
<span class="comment">% the entropy of the empirical distribution...</span>
samples_simulated = maxent.generateSamples(model,numel(idx_test));
simulated_empirical_distribution = maxent.getEmpiricalModel(samples_simulated);
fprintf(<span class="string">'Entropy of simulated data: %.03f bits\n'</span>,simulated_empirical_distribution.entropy);
</pre><pre class="codeoutput">Training to threshold: 1.500 standard deviations
Maximum samples: 17778   maximum MSE: 3.375
360/Inf samples=5411  MSE=6.481 (mean), 58.812 (max) [43]
434/Inf samples=11352  MSE=3.961 (mean), 59.231 (max) [2]
477/Inf samples=17437  MSE=2.877 (mean), 49.526 (max) [2]
511/Inf samples=17778  MSE=2.059 (mean), 31.914 (max) [40]
545/Inf samples=17778  MSE=1.761 (mean), 28.907 (max) [12]
581/Inf samples=17778  MSE=1.737 (mean), 15.406 (max) [13]
617/Inf samples=17778  MSE=1.581 (mean), 8.481 (max) [31]
652/Inf samples=17778  MSE=1.751 (mean), 11.608 (max) [410]
687/Inf samples=17778  MSE=1.579 (mean), 11.190 (max) [398]
722/Inf samples=17778  MSE=1.704 (mean), 8.920 (max) [1147]
756/Inf samples=17778  MSE=1.616 (mean), 10.387 (max) [12]
790/Inf samples=17778  MSE=1.516 (mean), 9.892 (max) [12]
824/Inf samples=17778  MSE=1.536 (mean), 4.847 (max) [12]
858/Inf samples=17778  MSE=1.595 (mean), 5.220 (max) [675]
892/Inf samples=17778  MSE=1.682 (mean), 5.710 (max) [1150]
926/Inf samples=17778  MSE=1.380 (mean), 4.979 (max) [1150]
960/Inf samples=17778  MSE=1.570 (mean), 4.877 (max) [590]
994/Inf samples=17778  MSE=1.439 (mean), 6.418 (max) [590]
1029/Inf samples=17778  MSE=1.485 (mean), 4.675 (max) [291]
1063/Inf samples=17778  MSE=1.460 (mean), 4.530 (max) [675]
1098/Inf samples=17778  MSE=1.366 (mean), 5.089 (max) [410]
1132/Inf samples=17778  MSE=1.390 (mean), 5.170 (max) [410]
1166/Inf samples=17778  MSE=1.353 (mean), 3.855 (max) [1225]
converged (marginals match)
Normalizing model...
[...............]
Model entropy: 17.689 bits, empirical entropy (test set): 11.940 bits
Entropy of simulated data: 12.485 bits
</pre><img vspace="5" hspace="5" src="maxent_example_02.png" alt=""> <img vspace="5" hspace="5" src="maxent_example_03.png" alt=""> <h2 id="4">part 3: working with RP (random projection) models</h2><pre class="codeinput"><span class="comment">% load spiking data of 15 neurons</span>
load <span class="string">example15</span>

<span class="comment">% randomly divide it into a training set and a test set (so we can verify how well we trained)</span>
[ncells,nsamples] = size(spikes15);
idx_train = randperm(nsamples,ceil(nsamples/2));
idx_test = setdiff(1:nsamples,idx_train);
samples_train = spikes15(:,idx_train);
samples_test = spikes15(:,idx_test);

<span class="comment">% create a random projection model with default settings</span>
model = maxent.createModel(ncells,<span class="string">'rp'</span>);

<span class="comment">% train the model to a threshold of one standard deviation from the error of computing the marginals.</span>
<span class="comment">% because the distribution is relatively small (15 dimensions) we can explicitly represent all 2^15 states</span>
<span class="comment">% in memory and train the model in an exhaustive fashion.</span>
model = maxent.trainModel(model,samples_train,<span class="string">'threshold'</span>,1);

<span class="comment">% now check the kullback-leibler divergence between the model predictions and the pattern counts in the test-set</span>
empirical_distribution = maxent.getEmpiricalModel(samples_test);
model_logprobs = maxent.getLogProbability(model,empirical_distribution.words);
test_dkl = maxent.dkl(empirical_distribution.logprobs,model_logprobs);
fprintf(<span class="string">'Kullback-Leibler divergence from test set: %f\n'</span>,test_dkl);

<span class="comment">% create a random projection model with a specified number of projections and specified sparsity</span>
model = maxent.createModel(ncells,<span class="string">'rp'</span>,<span class="string">'nprojections'</span>,500,<span class="string">'sparsity'</span>,0.1);

<span class="comment">% train the model</span>
model = maxent.trainModel(model,samples_train,<span class="string">'threshold'</span>,1);

<span class="comment">% now check the kullback-leibler divergence between the model predictions and the pattern counts in the test-set</span>
empirical_distribution = maxent.getEmpiricalModel(samples_test);
model_logprobs = maxent.getLogProbability(model,empirical_distribution.words);
test_dkl = maxent.dkl(empirical_distribution.logprobs,model_logprobs);
fprintf(<span class="string">'Kullback-Leibler divergence from test set: %f\n'</span>,test_dkl);
</pre><pre class="codeoutput">Training to threshold: 1.000 standard deviations
Maximum MSE: 1.000
62/Inf  MSE=0.132 (mean), 24.269 (max) [1]  DKL: 0.130
134/Inf  MSE=0.014 (mean), 7.109 (max) [1]  DKL: 0.116
207/Inf  MSE=0.004 (mean), 2.248 (max) [1]  DKL: 0.113
281/Inf  MSE=0.002 (mean), 1.304 (max) [1]  DKL: 0.111
converged (marginals match)
Standard deviations from marginals: 0.037 (mean), 0.996 (max) [1]  DKL: 0.111
Kullback-Leibler divergence from test set: 0.115675
Training to threshold: 1.000 standard deviations
Maximum MSE: 1.000
11/Inf  MSE=5.037 (mean), 528.439 (max) [35]  DKL: 0.156
29/Inf  MSE=0.089 (mean), 7.456 (max) [287]  DKL: 0.117
47/Inf  MSE=0.024 (mean), 5.721 (max) [287]  DKL: 0.109
64/Inf  MSE=0.009 (mean), 1.851 (max) [194]  DKL: 0.106
converged (marginals match)
Standard deviations from marginals: 0.076 (mean), 0.992 (max) [194]  DKL: 0.105
Kullback-Leibler divergence from test set: 0.107917
</pre><h2 id="5">part 4: specifying a custom list of correlations</h2><pre class="codeinput"><span class="comment">% load spiking data of 15 neurons</span>
load <span class="string">example15</span>

<span class="comment">% randomly divide it into a training set and a test set (so we can verify how well we trained)</span>
[ncells,nsamples] = size(spikes15);
idx_train = randperm(nsamples,ceil(nsamples/2));
idx_test = setdiff(1:nsamples,idx_train);
samples_train = spikes15(:,idx_train);
samples_test = spikes15(:,idx_test);


<span class="comment">% create a model with first, second, and third-order correlations. (third-order model)</span>
<span class="comment">% we will do this by specifying a list of all the possible combinations of single factors, pairs and triplets</span>
correlations = cat(1,num2cell(nchoosek(1:ncells,1),2), <span class="keyword">...</span>
                num2cell(nchoosek(1:ncells,2),2),<span class="keyword">...</span>
                num2cell(nchoosek(1:ncells,3),2));

model = maxent.createModel(ncells,<span class="string">'highorder'</span>,correlations);

<span class="comment">% train it</span>
model = maxent.trainModel(model,samples_train,<span class="string">'threshold'</span>,1);

<span class="comment">% use the model to predict the frequency of activity patterns.</span>
<span class="comment">% We will start by observing all the patterns that repeated at least twice (because a pattern that repeated at least</span>
<span class="comment">% once may grossly overrepresent its probability and is not meaningful in this sort of analysis)</span>
limited_empirical_distribution = maxent.getEmpiricalModel(samples_test,<span class="string">'min_count'</span>,2);

<span class="comment">% get the model predictions for these patterns</span>
model_logprobs = maxent.getLogProbability(model,limited_empirical_distribution.words);

<span class="comment">% nplot on a log scale</span>
figure
plot(limited_empirical_distribution.logprobs,model_logprobs,<span class="string">'bo'</span>);
hold <span class="string">on</span>;
minval = min(limited_empirical_distribution.logprobs);
plot([minval 0],[minval 0],<span class="string">'-r'</span>);  <span class="comment">% identity line</span>
xlabel(<span class="string">'empirical pattern log frequency'</span>);
ylabel(<span class="string">'predicted pattern log frequency'</span>);
title(sprintf(<span class="string">'Third order model: activity patterns in %d cells'</span>,ncells));
</pre><pre class="codeoutput">Training to threshold: 1.000 standard deviations
Maximum MSE: 1.000
25/Inf  MSE=4.135 (mean), 637.538 (max) [12]  DKL: 0.299
53/Inf  MSE=1.394 (mean), 41.939 (max) [8]  DKL: 0.151
82/Inf  MSE=0.574 (mean), 15.810 (max) [13]  DKL: 0.112
111/Inf  MSE=0.170 (mean), 22.392 (max) [2]  DKL: 0.096
139/Inf  MSE=0.118 (mean), 11.623 (max) [12]  DKL: 0.091
168/Inf  MSE=0.060 (mean), 3.858 (max) [1]  DKL: 0.088
196/Inf  MSE=0.069 (mean), 5.619 (max) [215]  DKL: 0.086
224/Inf  MSE=0.043 (mean), 2.878 (max) [501]  DKL: 0.085
252/Inf  MSE=0.021 (mean), 1.336 (max) [215]  DKL: 0.083
converged (marginals match)
Standard deviations from marginals: 0.140 (mean), 0.995 (max) [215]  DKL: 0.083
</pre><img vspace="5" hspace="5" src="maxent_example_04.png" alt=""> <h2 id="6">part 5: constructing composite models</h2><pre class="codeinput"><span class="comment">% load spiking data of 15 neurons</span>
load <span class="string">example15</span>

<span class="comment">% randomly divide it into a training set and a test set (so we can verify how well we trained)</span>
[ncells,nsamples] = size(spikes15);
idx_train = randperm(nsamples,ceil(nsamples/2));
idx_test = setdiff(1:nsamples,idx_train);
samples_train = spikes15(:,idx_train);
samples_test = spikes15(:,idx_test);


<span class="comment">% create a model with independent factors, k-synchrony and third-order correlations</span>
<span class="comment">% we will do this by initializing 3 separate models and then combining them to a single model</span>
third_order_correlations = num2cell(nchoosek(1:ncells,3),2);
model_indep = maxent.createModel(ncells,<span class="string">'indep'</span>);
model_ksync = maxent.createModel(ncells,<span class="string">'ksync'</span>);
model_thirdorder = maxent.createModel(ncells,<span class="string">'highorder'</span>,third_order_correlations);
model = maxent.createModel(ncells,<span class="string">'composite'</span>,{model_indep,model_ksync,model_thirdorder});

<span class="comment">% train it</span>
model = maxent.trainModel(model,samples_train,<span class="string">'threshold'</span>,1);

<span class="comment">% use the model to predict the frequency of activity patterns.</span>
<span class="comment">% We will start by observing all the patterns that repeated at least twice (because a pattern that repeated at least</span>
<span class="comment">% once may grossly overrepresent its probability and is not meaningful in this sort of analysis)</span>
limited_empirical_distribution = maxent.getEmpiricalModel(samples_test,<span class="string">'min_count'</span>,2);

<span class="comment">% get the model predictions for these patterns</span>
model_logprobs = maxent.getLogProbability(model,limited_empirical_distribution.words);

<span class="comment">% nplot on a log scale</span>
figure
plot(limited_empirical_distribution.logprobs,model_logprobs,<span class="string">'bo'</span>);
hold <span class="string">on</span>;
minval = min(limited_empirical_distribution.logprobs);
plot([minval 0],[minval 0],<span class="string">'-r'</span>);  <span class="comment">% identity line</span>
xlabel(<span class="string">'empirical pattern log frequency'</span>);
ylabel(<span class="string">'predicted pattern log frequency'</span>);
title(sprintf(<span class="string">'Composite model: activity patterns in %d cells'</span>,ncells));
</pre><pre class="codeoutput">Training to threshold: 1.000 standard deviations
Maximum MSE: 1.000
30/Inf  MSE=0.468 (mean), 14.750 (max) [12]  DKL: 0.119
63/Inf  MSE=0.156 (mean), 3.632 (max) [194]  DKL: 0.108
96/Inf  MSE=0.061 (mean), 2.847 (max) [150]  DKL: 0.104
129/Inf  MSE=0.029 (mean), 1.941 (max) [25]  DKL: 0.101
162/Inf  MSE=0.024 (mean), 5.039 (max) [24]  DKL: 0.100
195/Inf  MSE=0.016 (mean), 2.325 (max) [24]  DKL: 0.099
converged (marginals match)
Standard deviations from marginals: 0.112 (mean), 1.000 (max) [24]  DKL: 0.098
</pre><img vspace="5" hspace="5" src="maxent_example_05.png" alt=""> <h2 id="7">part 6: constructing and sampling from high order Markov chains</h2><p>train a time-dependent model</p><pre class="codeinput"><span class="comment">% load spiking data of 15 neurons</span>
load <span class="string">example15_spatiotemporal</span>

ncells = size(spikes15_time_dependent,1);
history_length = 2;


<span class="comment">% Create joint words of (x_t-2,x_t-1,x_t).</span>
xt = [];
<span class="keyword">for</span> i = 1:(history_length+1)
   xt = [xt;spikes15_time_dependent(:,i:(end-history_length-1+i))];
<span class="keyword">end</span>


<span class="comment">% create a spatiotemporal model that works on series of binary words: (x_t-2,x_t-1,x_t).</span>
<span class="comment">% this essentially describes the probability distribution as a second-order Markov process.</span>
<span class="comment">% We will model the distribution with a composite model that uses firing rates, total synchrony in the last 3 time bins</span>
<span class="comment">% and pairwise correlations within the current time bin and pairwise correlations between the current activity of a cell</span>
<span class="comment">% and the previous time bin.</span>
time_dependent_ncells = ncells*(history_length+1);
inner_model_indep = maxent.createModel(time_dependent_ncells,<span class="string">'indep'</span>);  <span class="comment">% firing rates</span>
inner_model_ksync = maxent.createModel(time_dependent_ncells,<span class="string">'ksync'</span>);  <span class="comment">% total synchrony in the population</span>
<span class="comment">% add pairwise correlations only within the current time bin</span>
second_order_correlations = num2cell(nchoosek(1:ncells,2),2);
temporal_matrix = reshape(1:time_dependent_ncells,[ncells,history_length+1]);
temporal_interactions = [];
<span class="comment">% add pairwise correlations from the current time bin to the previous time step</span>
<span class="keyword">for</span> i = 1:(history_length)
    temporal_interactions = [temporal_interactions;temporal_matrix(:,[i,i+1])];
<span class="keyword">end</span>
<span class="comment">% bunch of all this together into one probabilistic model</span>
temporal_interactions = num2cell(temporal_interactions,2);
inner_model_pairwise = maxent.createModel(time_dependent_ncells,<span class="string">'highorder'</span>,[second_order_correlations;temporal_interactions]);
mspatiotemporal = maxent.createModel(time_dependent_ncells,<span class="string">'composite'</span>,{inner_model_indep,inner_model_ksync,inner_model_pairwise});

<span class="comment">% train the model on the concatenated words</span>
disp(<span class="string">'training spatio-temporal model...'</span>);
mspatiotemporal = maxent.trainModel(mspatiotemporal,xt);


<span class="comment">% sample from the model by generating each sample according to the history.</span>
<span class="comment">% for this we need to generate the 3n-dimensional samples one by one, each time fixing two-thirds of the code word</span>
<span class="comment">% corresponding to time t-2 and t-1 and sampling only from time t.</span>
disp(<span class="string">'sampling from spatio-temporal model...'</span>);
x0 = uint32(xt(:,1));
xspatiotemporal = [];
nsamples = 10000;
<span class="keyword">for</span> i = 1:nsamples

    <span class="comment">% get next sample. we will use burn-in of 100 each step to ensure that we don't introduce time-dependent stuff</span>
    <span class="comment">% related to the sampling process itself.</span>
    xnext = maxent.generateSamples(mspatiotemporal,1,<span class="string">'fix_indices'</span>,1:(ncells*history_length),<span class="string">'burnin'</span>,100,<span class="string">'x0'</span>,x0);
    generated_sample = xnext(((ncells*history_length)+1):end,1);

    <span class="comment">% shift the "current" state by one time step</span>
    x0 = [x0((ncells+1):end,:);generated_sample];

    <span class="comment">% add the current output</span>
    xspatiotemporal = [xspatiotemporal,generated_sample];

<span class="keyword">end</span>


<span class="comment">% plot the result</span>
display_begin = 2000;
nsamples_to_display = 300;


<span class="comment">% plot the actual raster</span>
pos = [400,600,700,250];
figure(<span class="string">'Position'</span>,pos);
subplot(2,1,1);
pos = get(gca, <span class="string">'Position'</span>);pos(1) = 0.055;pos(3) = 0.9;set(gca, <span class="string">'Position'</span>, pos);
imshow(~spikes15_time_dependent(:,display_begin+(1:nsamples_to_display)));
title(<span class="string">'Actual data'</span>);

<span class="comment">% plot raster sampled from a spatiotemporal model</span>
subplot(2,1,2);
pos = get(gca, <span class="string">'Position'</span>);pos(1) = 0.055;pos(3) = 0.9;set(gca, <span class="string">'Position'</span>, pos);
imshow(~xspatiotemporal(:,display_begin+(1:nsamples_to_display)));
title(<span class="string">'Synthetic data (2nd-order Markov)'</span>);
</pre><pre class="codeoutput">training spatio-temporal model...
Training to threshold: 1.300 standard deviations
Maximum samples: 47335   maximum MSE: 2.535
282/Inf samples=2463  MSE=19.636 (mean), 59.325 (max) [99]
357/Inf samples=5250  MSE=9.601 (mean), 27.690 (max) [95]
400/Inf samples=8082  MSE=6.189 (mean), 19.625 (max) [207]
430/Inf samples=10906  MSE=4.545 (mean), 13.930 (max) [49]
453/Inf samples=13723  MSE=3.734 (mean), 13.003 (max) [99]
472/Inf samples=16589  MSE=3.225 (mean), 12.871 (max) [99]
488/Inf samples=19459  MSE=2.753 (mean), 10.577 (max) [99]
502/Inf samples=22374  MSE=2.366 (mean), 7.886 (max) [99]
514/Inf samples=25217  MSE=2.080 (mean), 6.888 (max) [101]
525/Inf samples=28140  MSE=1.899 (mean), 6.971 (max) [101]
535/Inf samples=31088  MSE=1.725 (mean), 6.727 (max) [212]
544/Inf samples=34005  MSE=1.569 (mean), 7.961 (max) [212]
553/Inf samples=37195  MSE=1.435 (mean), 8.451 (max) [212]
561/Inf samples=40280  MSE=1.339 (mean), 8.847 (max) [212]
568/Inf samples=43189  MSE=1.235 (mean), 8.667 (max) [212]
575/Inf samples=46307  MSE=1.166 (mean), 8.193 (max) [212]
582/Inf samples=47335  MSE=1.105 (mean), 8.121 (max) [212]
589/Inf samples=47335  MSE=1.045 (mean), 6.708 (max) [212]
596/Inf samples=47335  MSE=1.004 (mean), 6.391 (max) [212]
603/Inf samples=47335  MSE=0.983 (mean), 5.160 (max) [212]
610/Inf samples=47335  MSE=0.959 (mean), 3.869 (max) [212]
617/Inf samples=47335  MSE=0.970 (mean), 3.073 (max) [105]
624/Inf samples=47335  MSE=0.950 (mean), 2.744 (max) [105]
631/Inf samples=47335  MSE=0.932 (mean), 2.706 (max) [105]
converged (marginals match)
sampling from spatio-temporal model...
</pre><img vspace="5" hspace="5" src="maxent_example_06.png" alt=""> <p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2017b</a><br></p></div><!--
##### SOURCE BEGIN #####
% maxent_example.m
%
% Example code for the maximum entropy toolkit
% Ori Maoz, July 2016:  orimaoz@gmail.com,


%
% Note: all the "maxent." prefixes before the function calls can be omitted by commenting out the following line:
%import maxent.*

%% part 1: working with small distributions of neurons (exhaustively)

% load spiking data of 15 neurons
load example15

% randomly divide it into a training set and a test set (so we can verify how well we trained)
[ncells,nsamples] = size(spikes15);
idx_train = randperm(nsamples,ceil(nsamples/2));
idx_test = setdiff(1:nsamples,idx_train);
samples_train = spikes15(:,idx_train);
samples_test = spikes15(:,idx_test);

% create a k-pairwise model (pairwise maxent with synchrony constraints)
model = maxent.createModel(ncells,'kising');

% train the model to a threshold of one standard deviation from the error of computing the marginals.
% because the distribution is relatively small (15 dimensions) we can explicitly represent all 2^15 states 
% in memory and train the model in an exhaustive fashion.
model = maxent.trainModel(model,samples_train,'threshold',1);

% now check the kullback-leibler divergence between the model predictions and the pattern counts in the test-set 
empirical_distribution = maxent.getEmpiricalModel(samples_test);
model_logprobs = maxent.getLogProbability(model,empirical_distribution.words);
test_dkl = maxent.dkl(empirical_distribution.logprobs,model_logprobs);
fprintf('Kullback-Leibler divergence from test set: %f\n',test_dkl);

model_entropy = maxent.getEntropy(model);
fprintf('Model entropy: %.03f   empirical dataset entropy: %.03f\n', model_entropy, empirical_distribution.entropy);

% get the marginals (firing rates and correlations) of the test data and see how they compare to the model predictions
marginals_data = maxent.getEmpiricalMarginals(samples_test,model);
marginals_model = maxent.getMarginals(model);

% plot them on a log scale
figure
loglog(marginals_data,marginals_model,'b*');
hold on;
minval = min([marginals_data(marginals_data>0)]);
plot([minval 1],[minval 1],'-r'); % identity line
xlabel('empirical marginal');
ylabel('predicted marginal');
title(sprintf('marginals in %d cells',ncells));


%% part 2: working with larger distributions of neurons (MCMC)

load example50

% randomly divide into train/test sets
[ncells,nsamples] = size(spikes50);
idx_train = randperm(nsamples,ceil(nsamples/2));
idx_test = setdiff(1:nsamples,idx_train);
samples_train = spikes50(:,idx_train);
samples_test = spikes50(:,idx_test);

% create a pairwise maximum entropy model
model = maxent.createModel(50,'pairwise');

% train the model to a threshold of 1.5 standard deviations from the error of computing the marginals.
% because the distribution is larger (50 dimensions) we cannot explicitly iterate over all 5^20 states 
% in memory and will use markov chain monte carlo (MCMC) methods to obtain an approximation
model = maxent.trainModel(model,samples_train,'threshold',1.5);


% get the marginals (firing rates and correlations) of the test data and see how they compare to the model predictions.
% here the model marginals could not be computed exactly so they will be estimated using monte-carlo. We specify the
% number of samples we use so that their estimation will have the same amoutn noise as the empirical marginal values
marginals_data = maxent.getEmpiricalMarginals(samples_test,model);
marginals_model = maxent.getMarginals(model,'nsamples',size(samples_test,2));

% plot them on a log scale
figure
loglog(marginals_data,marginals_model,'b*');
hold on;
minval = min([marginals_data(marginals_data>0)]);
plot([minval 1],[minval 1],'-r'); % identity line
xlabel('empirical marginal');
ylabel('predicted marginal');
title(sprintf('marginals in %d cells',ncells));

% the model that the MCMC solver returns is not normalized. If we want to compare the predicted and actual probabilities
% of individual firing patterns, we will need to first normalize the model. We will use the wang-landau algorithm for
% this. We chose parameters which are less strict than the default settings so that we will have a faster runtime.
disp('Normalizing model...');
model = maxent.wangLandau(model,'binsize',0.1,'depth',15);

% the normalization factor was added to the model structure. Now that we have a normalized model, we'll use it to
% predict the frequency of activity patterns. We will start by observing all the patterns that repeated at least twice
% (because a pattern that repeated at least once may grossly overrepresent its probability and is not meaningful in this
% sort of analysis)
limited_empirical_distribution = maxent.getEmpiricalModel(samples_test,'min_count',2);


% get the model predictions for these patterns
model_logprobs = maxent.getLogProbability(model,limited_empirical_distribution.words);

% nplot on a log scale
figure
plot(limited_empirical_distribution.logprobs,model_logprobs,'bo');
hold on;
minval = min(limited_empirical_distribution.logprobs);
plot([minval 0],[minval 0],'-r');  % identity line
xlabel('empirical pattern log frequency');
ylabel('predicted pattern log frequency');
title(sprintf('activity pattern frequency in %d cells',ncells));



% Wang-landau also approximated the model entropy, let's compare it to the entropy of the empirical dataset.
% for this we want to look at the entire set, not just the set limited repeating patterns
empirical_distribution = maxent.getEmpiricalModel(samples_test);

% it will not be surprising to see that the empirical entropy is much lower than the model, this is because the
% distribution is very undersampled
fprintf('Model entropy: %.03f bits, empirical entropy (test set): %.03f bits\n',model.entropy,empirical_distribution.entropy);

% generate samples from the distribution and compute their entropy. This should give a result which is must closer to
% the entropy of the empirical distribution...
samples_simulated = maxent.generateSamples(model,numel(idx_test));
simulated_empirical_distribution = maxent.getEmpiricalModel(samples_simulated);
fprintf('Entropy of simulated data: %.03f bits\n',simulated_empirical_distribution.entropy);



%% part 3: working with RP (random projection) models

% load spiking data of 15 neurons
load example15

% randomly divide it into a training set and a test set (so we can verify how well we trained)
[ncells,nsamples] = size(spikes15);
idx_train = randperm(nsamples,ceil(nsamples/2));
idx_test = setdiff(1:nsamples,idx_train);
samples_train = spikes15(:,idx_train);
samples_test = spikes15(:,idx_test);

% create a random projection model with default settings
model = maxent.createModel(ncells,'rp');

% train the model to a threshold of one standard deviation from the error of computing the marginals.
% because the distribution is relatively small (15 dimensions) we can explicitly represent all 2^15 states 
% in memory and train the model in an exhaustive fashion.
model = maxent.trainModel(model,samples_train,'threshold',1);

% now check the kullback-leibler divergence between the model predictions and the pattern counts in the test-set 
empirical_distribution = maxent.getEmpiricalModel(samples_test);
model_logprobs = maxent.getLogProbability(model,empirical_distribution.words);
test_dkl = maxent.dkl(empirical_distribution.logprobs,model_logprobs);
fprintf('Kullback-Leibler divergence from test set: %f\n',test_dkl);

% create a random projection model with a specified number of projections and specified sparsity
model = maxent.createModel(ncells,'rp','nprojections',500,'sparsity',0.1);

% train the model
model = maxent.trainModel(model,samples_train,'threshold',1);

% now check the kullback-leibler divergence between the model predictions and the pattern counts in the test-set 
empirical_distribution = maxent.getEmpiricalModel(samples_test);
model_logprobs = maxent.getLogProbability(model,empirical_distribution.words);
test_dkl = maxent.dkl(empirical_distribution.logprobs,model_logprobs);
fprintf('Kullback-Leibler divergence from test set: %f\n',test_dkl);


%% part 4: specifying a custom list of correlations

% load spiking data of 15 neurons
load example15

% randomly divide it into a training set and a test set (so we can verify how well we trained)
[ncells,nsamples] = size(spikes15);
idx_train = randperm(nsamples,ceil(nsamples/2));
idx_test = setdiff(1:nsamples,idx_train);
samples_train = spikes15(:,idx_train);
samples_test = spikes15(:,idx_test);


% create a model with first, second, and third-order correlations. (third-order model)
% we will do this by specifying a list of all the possible combinations of single factors, pairs and triplets
correlations = cat(1,num2cell(nchoosek(1:ncells,1),2), ...
                num2cell(nchoosek(1:ncells,2),2),...
                num2cell(nchoosek(1:ncells,3),2));

model = maxent.createModel(ncells,'highorder',correlations);
            
% train it
model = maxent.trainModel(model,samples_train,'threshold',1);

% use the model to predict the frequency of activity patterns. 
% We will start by observing all the patterns that repeated at least twice (because a pattern that repeated at least 
% once may grossly overrepresent its probability and is not meaningful in this sort of analysis)
limited_empirical_distribution = maxent.getEmpiricalModel(samples_test,'min_count',2);

% get the model predictions for these patterns
model_logprobs = maxent.getLogProbability(model,limited_empirical_distribution.words);

% nplot on a log scale
figure
plot(limited_empirical_distribution.logprobs,model_logprobs,'bo');
hold on;
minval = min(limited_empirical_distribution.logprobs);
plot([minval 0],[minval 0],'-r');  % identity line
xlabel('empirical pattern log frequency');
ylabel('predicted pattern log frequency');
title(sprintf('Third order model: activity patterns in %d cells',ncells));


%% part 5: constructing composite models


% load spiking data of 15 neurons
load example15

% randomly divide it into a training set and a test set (so we can verify how well we trained)
[ncells,nsamples] = size(spikes15);
idx_train = randperm(nsamples,ceil(nsamples/2));
idx_test = setdiff(1:nsamples,idx_train);
samples_train = spikes15(:,idx_train);
samples_test = spikes15(:,idx_test);


% create a model with independent factors, k-synchrony and third-order correlations
% we will do this by initializing 3 separate models and then combining them to a single model
third_order_correlations = num2cell(nchoosek(1:ncells,3),2);
model_indep = maxent.createModel(ncells,'indep');
model_ksync = maxent.createModel(ncells,'ksync');
model_thirdorder = maxent.createModel(ncells,'highorder',third_order_correlations);
model = maxent.createModel(ncells,'composite',{model_indep,model_ksync,model_thirdorder});
            
% train it
model = maxent.trainModel(model,samples_train,'threshold',1);

% use the model to predict the frequency of activity patterns. 
% We will start by observing all the patterns that repeated at least twice (because a pattern that repeated at least 
% once may grossly overrepresent its probability and is not meaningful in this sort of analysis)
limited_empirical_distribution = maxent.getEmpiricalModel(samples_test,'min_count',2);

% get the model predictions for these patterns
model_logprobs = maxent.getLogProbability(model,limited_empirical_distribution.words);

% nplot on a log scale
figure
plot(limited_empirical_distribution.logprobs,model_logprobs,'bo');
hold on;
minval = min(limited_empirical_distribution.logprobs);
plot([minval 0],[minval 0],'-r');  % identity line
xlabel('empirical pattern log frequency');
ylabel('predicted pattern log frequency');
title(sprintf('Composite model: activity patterns in %d cells',ncells));


%% part 6: constructing and sampling from high order Markov chains
% train a time-dependent model

% load spiking data of 15 neurons
load example15_spatiotemporal

ncells = size(spikes15_time_dependent,1);
history_length = 2;


% Create joint words of (x_t-2,x_t-1,x_t).
xt = [];
for i = 1:(history_length+1)   
   xt = [xt;spikes15_time_dependent(:,i:(end-history_length-1+i))];
end


% create a spatiotemporal model that works on series of binary words: (x_t-2,x_t-1,x_t).
% this essentially describes the probability distribution as a second-order Markov process.
% We will model the distribution with a composite model that uses firing rates, total synchrony in the last 3 time bins
% and pairwise correlations within the current time bin and pairwise correlations between the current activity of a cell
% and the previous time bin.
time_dependent_ncells = ncells*(history_length+1);
inner_model_indep = maxent.createModel(time_dependent_ncells,'indep');  % firing rates 
inner_model_ksync = maxent.createModel(time_dependent_ncells,'ksync');  % total synchrony in the population
% add pairwise correlations only within the current time bin
second_order_correlations = num2cell(nchoosek(1:ncells,2),2);
temporal_matrix = reshape(1:time_dependent_ncells,[ncells,history_length+1]);
temporal_interactions = [];
% add pairwise correlations from the current time bin to the previous time step
for i = 1:(history_length)
    temporal_interactions = [temporal_interactions;temporal_matrix(:,[i,i+1])];
end
% bunch of all this together into one probabilistic model
temporal_interactions = num2cell(temporal_interactions,2);
inner_model_pairwise = maxent.createModel(time_dependent_ncells,'highorder',[second_order_correlations;temporal_interactions]);
mspatiotemporal = maxent.createModel(time_dependent_ncells,'composite',{inner_model_indep,inner_model_ksync,inner_model_pairwise});

% train the model on the concatenated words
disp('training spatio-temporal model...');
mspatiotemporal = maxent.trainModel(mspatiotemporal,xt);


% sample from the model by generating each sample according to the history.
% for this we need to generate the 3n-dimensional samples one by one, each time fixing two-thirds of the code word
% corresponding to time t-2 and t-1 and sampling only from time t. 
disp('sampling from spatio-temporal model...');
x0 = uint32(xt(:,1));
xspatiotemporal = [];
nsamples = 10000;
for i = 1:nsamples
    
    % get next sample. we will use burn-in of 100 each step to ensure that we don't introduce time-dependent stuff
    % related to the sampling process itself.
    xnext = maxent.generateSamples(mspatiotemporal,1,'fix_indices',1:(ncells*history_length),'burnin',100,'x0',x0);    
    generated_sample = xnext(((ncells*history_length)+1):end,1);

    % shift the "current" state by one time step
    x0 = [x0((ncells+1):end,:);generated_sample];
    
    % add the current output 
    xspatiotemporal = [xspatiotemporal,generated_sample];
    
end


% plot the result
display_begin = 2000;
nsamples_to_display = 300;

    
% plot the actual raster
pos = [400,600,700,250];
figure('Position',pos);
subplot(2,1,1);
pos = get(gca, 'Position');pos(1) = 0.055;pos(3) = 0.9;set(gca, 'Position', pos);
imshow(~spikes15_time_dependent(:,display_begin+(1:nsamples_to_display)));
title('Actual data');

% plot raster sampled from a spatiotemporal model
subplot(2,1,2);
pos = get(gca, 'Position');pos(1) = 0.055;pos(3) = 0.9;set(gca, 'Position', pos);
imshow(~xspatiotemporal(:,display_begin+(1:nsamples_to_display)));
title('Synthetic data (2nd-order Markov)');


##### SOURCE END #####
--></body></html>